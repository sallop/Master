\documentclass[a4paper]{jarticle} 
\usepackage[dvips,usenames]{color} 
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{bm}  % for bmdefine
% \pagestyle{empty}
\topmargin = 0mm
\oddsidemargin = 5mm
\textwidth = 152mm
\textheight = 240mm
\pagestyle{fancyplain}
\renewcommand{\footrulewidth}{0pt}
\renewcommand{\headrulewidth}{0pt}
\bmdefine{\bx}{x}
\bmdefine{\bv}{v}
\bmdefine{\bh}{h}
\begin{document}
\lhead{\small 情報システム工学科}
\rhead{\small 2009年12月14日\\塩貝亮宇}

% \cfoot{}
%
%\begin{flushright}%
%2006年 9月 28日
%\end{flushright}
%

\centerline{\Large\gt \LaTeX を使ったレポート作成}      
\vskip 5mm
\centerline{\Large\gt T1076006 塩貝亮宇}


\section{An overview of Restricted Boltzmann Machines and Contrastive Divergence} 

訓練セットを意味解釈の目的でバイナリ画像を担うバイナリベクタとして考える.

consider A of B

訓練セットは{\rm Restricted Boltzmann Machine}と呼ばれるネットワークを用いて
モデル化できるが,これは確率的,バイナリ画素値は確率的に結合され,
対照的に重み付けされた結合を用いるバイナリ特徴検出器.


画素は{\rm RBM}の可視素子と一致する.なぜならば,それらの状態は観測される;
特徴検出器は隠れ素子と一致する.
可視素子と隠れ素子の同時確率$(\bv,\bh)$({\rm joint configuration})は
エネルギー持ち,次のように与えられる.
\begin{eqnarray}
 E(\bv,\bh) = -\sum_{} a_i v_i - \sum b_j h_j - \sum v_i h_j w_{ij}
\end{eqnarray}

ここで,$v_i,h_j$は可視素子$i$と隠れ素子$j$のバイナリ状態で,
$a_i,b_j$は閾値と$w_{ij}$はそれらの重みとなる.

ネットワークはエネルギー関数をつかって
この可視素子と隠れ素子のとりえる全ての組を用いて
割り当てられ：
\begin{eqnarray}
 p(\bv,\bh) = \frac{1}{Z} exp(-E(\bv,\bh))
\end{eqnarray}

ここで,'分配関数',Z,は隠れ素子と可視素子の取りえる全ての和によって与えら
れ,
\begin{eqnarray}
 Z = \sum_{\bv,\bh} exp(-E(\bv,\bh))
\end{eqnarray}

可視ベクトル$\bv$によって割り当てられるネットワークの確率は
隠れ素子の取りえる全ての和によって与えられ,
\begin{eqnarray}
 p(\bv) = \frac{1}{Z} \sum_{\bh} exp(-E(\bv,\bh))
\end{eqnarray}

ネットワークに訓練画像を与えることによって確率は

調整された重みと画像のエネルギーの下限のバイアスと他のイメージのエネルギーを上げるために??

とくに,エネルギーの下をもち,そして,


そして,従って分配関数に大きく寄与する.

訓練ベクトルの対数確率の重みに対する微分は驚くほどシンブルだ.
\begin{eqnarray}
 \frac{\partial log p(\bv)}{\partial w_{ij}} 
  = <v_i h_j>_{data} - <v_i h_j>_{model}
\end{eqnarray}

ここで,山括弧は下付き文字によって指定される分布による期待値を示している.


これは訓練データの対数確率で実行できる確率的な傾斜について,
とてもシンプルな学習則を導く.

\begin{eqnarray}
 \Delta w_{ij} = \epsilon( <v_i h_j>_{data} - <v_i h_j>_{model})
\end{eqnarray}
ここで,$\epsilon$は学習率を現わす.

なぜならば,それらは{\rm RBM}の隠れ素子間で直接の結合を持たず,
バイアスの掛っていない$<v_i h_j>_data$のサンプルを容易に得ることができる.
訓練データ$\bv$をランダムに与えることで,
それぞれの隠れ素子$j$の$h_j$のバイナリ状態は$1$が固定され

\begin{eqnarray}
 p(h_j = 1 | \bv) = \sigma(b_j + \sum_{j} h_j w_{ij})
\end{eqnarray}

ここで,$\sigma(x)$はロジスティックシグモイド関数$1/ (1 + exp(-x))$.

$v_i h_j$はバイアスの掛っていないサンプル.

なぜならば,{\rm RBM}は可視素子間に結合を持たず,
可視素子の状態のバイアスの掛からないサンプルもまた容易に得ることができ,
隠れベクトルは次のように与えられる.

\begin{eqnarray}
 p(v_i = 1 | \bh) = \sigma(a_i + \sum_{j} h_j w_{ij})
\end{eqnarray}

$<v_i,h_j>_{model}$のバイアスの掛かっていないサンプルを得ることは,
しかしながら,これはとても難しい.

これは,可視素子のどんなランダムな状態から始めても,そして,
とても長い時間において交互にギブスサンプリングを実行する.
It can be done by starting at random state of the visible units 
and performing alternating Gibbs sampling for a very long time.



ギブスサンプリングの一回の反復は
式(7)を用いて隠れ素子の全てが並行に更新され,
続いて式(8)を用いて可視素子の全てが並行に更新されるところ
からなる.


{\rm Hinton}の提案により,より高速な学習手続きが提案された(2002).


訓練データを可視素子の状態にセットするところから始まる.

その結果,全ての隠れ素子のバイナリ状態は式(7)を用いて並行に計算される.

一度,隠れ素子についてバイナリ状態を選択すると,
`reconstruction`は式(8)によって与えられる確率のそれぞれの$v_i$に{\rm 1}
を代入することで生み出される.
重みの変更は次式によって与えられ,
\begin{eqnarray}
 \Delta w_{ij} = \epsilon(<v_i h_j>_{data} - <v_i h_j>_{recon})
\end{eqnarray}


対となる生成の代りに個々の素子の状態を使う簡易化された同様の学習則
はバイアスについて用いられる.

この学習は

訓練データの対数確率の勾配の大雑把な概算だけよく働く.(Hinton, 2002)

学習則は

2つの{\rm Kullback-Liebler divergence}間の差である{\rm Contrastive
Divergence}と呼ばれる別の目的関数の勾配の近似はより学習則に近い.
しかし,これは目的関数内の1つの巧妙な項を無視し,
だから,以下の勾配ですら続かない.

実際に,{\rm Sutskever}と{\rm Tieleman}は

どんな関数の勾配にも続かないことを示した.(Sutskever and Tieleman,2010).

それにも関わらず,
これは

するために十分に良く働く.

成功に到達するまでの多くの重要な適応は


{\rm RBMs}は一般に良い学習モデルだ.
もし,学習則の2番目の項について統計的にギブスサンプリングを集めない,
負の確率と呼ばれるだろう.

$CD_n$は
交互におこなわれるギブスサンプリングが$n$回ほど
完全に使われる学習を示すのに使われるだろう.









\end{document}


