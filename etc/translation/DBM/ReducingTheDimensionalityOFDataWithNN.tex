\documentclass[a4paper]{jarticle} 
\usepackage[dvips,usenames]{color} 
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{bm}  % for bmdefine
% \pagestyle{empty}
\topmargin = 0mm
\oddsidemargin = 5mm
\textwidth = 152mm
\textheight = 240mm
\pagestyle{fancyplain}
\renewcommand{\footrulewidth}{0pt}
\renewcommand{\headrulewidth}{0pt}
\bmdefine{\bx}{x}
\bmdefine{\bv}{v}
\bmdefine{\bh}{h}
\makeatletter
\newcommand{\figcaption}[1]{\def\@captype{figure}\caption{#1}}
\newcommand{\tblcaption}[1]{\def\@captype{table}\caption{#1}}
\makeatother

\begin{document}
\lhead{\small 情報システム工学科}
\rhead{\small 2009年4月12日\\塩貝亮宇}
% \cfoot{}
%
%\begin{flushright}%
%2006年 9月 28日
%\end{flushright}
%

\centerline{\Large\gt Reducing the Dimensionality of Data with Neural Networks}

\vskip 5mm
\centerline{\Large\gt T1076006 塩貝亮宇}

\section{title 1}
高次元の入力ベクトルを復元するために、
小さい中間層を伴う多層ニューラルネットによって訓練される低次元コードで、
高次元データは変換することが可能である。

勾配降下は、そのような'autoencoder'網で
'fine-tuning'する重みについて使用することが可能だが、
これは初期の重みが良い解答に近いときだけよく働く。

データの次元を減少させるための道具として、
主要な構成を解析するためのより良い低次元コードを学習するために
{\rm deep autoencoder network}が
重みを初期化する効果的な方法について論述する。

\section{paragraph 1} 
次元の減少は、可視化、コミュニケーション、
高次元データの(格納、storage)を手助けする。

シンプルで幅広く使われている手法は主成分分析(PCA)で、
これはデータセットの最良な分散の方向を発見し、そして、
それらの方向の各々に沿った(次元,軸,coordinate)により表現される。

高次元データを低次元データへ変換するための
多層'encoder'ネットワークと同種な'decoder'ネットワークを
コードから再生する非線形{\rm PCA}の一般化について説明していく。

2つのネットワークはランダムな重みで開始され、
それらはオリジナルデータとその復元の間の
相違を最小化することにより同時に訓練できる。

必要とされる勾配降下は最初に'decoder'ネットワークを通過し、
そして、そのとき'encoder'ネットワーク通過する
誤差逆伝搬エラーの(派生、導関数、derivative)へ向かう
チェインルールを用いることで容易に獲得される(1)。

全てのシステムは'autoencoder'と呼ばれ、そして、図(1)に描かれる。

\section{paragraph 2} 

多層の隱れ層を持つ非線形'autoencoder'の重みを最適することは難しい。

広範の初期の重みについて、
'autoencoder'は一般的に(貧しい,貧弱な,poor)局所最適解を発見し;
小さい初期化重みを伴ない、最初の層はの勾配はとても小さく、
多くの隱れ層を伴なって'autoencoder'を訓練することは実行不可能だ。

もし、初期の重みが良い解に近ければ
(勾配降下,gradient descent)は良く働き、

しかし、そのような初期の重みを発見することは

とても異なる型の特徴の(ひとつ、ある、one)層を
同時に学習するアルゴリズムを必要とする。

我々はバイナリデータについて、この'事前訓練'手続きを紹介し、
これを実数データに向けて一般化し、そして、
データセットの多様性ついてよく働くことを示す。


\section{paragraph 3} 
バイナリベクトルの集合体(例えば、画像)は
'restricted Boltzmann machine'と呼ばれる
2層のネットワークを使うことでモデル化され、
バイナリピクセルは統計的に結合されており、
バイナリ特徴は対称結合の重みが使われる。

ピクセルは{\rm RBM}の可視素子と一致する。
なぜならば、それらの状態は観測され；
その特徴検出器は'hidden'素子と一致する。

可視素子と隱れ素子の間の$(\bv,\bh)$の
(joint configuration,関節配置)が持つエネルギーは
\begin{eqnarray}
 E(\bv,\bh) &=&
  -\sum_{i \in pixels} b_i v_i
  -\sum_{j \in features} b_j h_j
  -\sum_{i,j} v_i h_j w_{ij}
\end{eqnarray}
によって与えられ、
ここで、$v_i$と$h_j$はピクセル$i$と特徴$j$のバイナリ状態、
$b_i$と$b_j$はそれらのバイアス、
$w_{ij}$はそれらの間の重みである。

ネットワークには、エネルギー関数を通して全ての取りうる画像が代入され、
(8)のように説明される。

訓練画像の確率は

重みとバイアス

調整する下限へ向うその画像のエネルギーと
類似するエネルギーを上げるために、

ネットワークよりむしろ真値データを選択するだろう
'(confabulated,談笑される)'画像。


与えられた訓練画像、それぞれ特徴検出器$j$のバイナリ状態$h_j$には
確率$\sigma(b_j + \sum_{i} v_i w_{ij})$に伴ない$1$が代入されるが、
ここで$\sigma(x)$はロジスティック関数$1/[1+\exp(-x)]$、
$b_j$は$j$のバイアス、$v_i$は画素$i$の状態、
そして$w_{ij}$は$i$と$j$の間の重みだ。

(かつて、いちど、once)、
バイナリ状態は隱れ素子について選択されており、
'confabulation(談笑)'は
各$v_i$に確率$\sigma(b_j + \sum_{j} h_j w_{ij})$に
伴なって$1$が代入されるが、ここで$b_i$は$i$のバイアスである。

その隱れ素子の状態は、
もう一度繰り返して、
それらが(談笑,confabulation)の特徴を表現できた
そのときに更新される。

その重みの変化は
\begin{eqnarray}
 \Delta w_{ij} &=& \epsilon(
  \langle v_i h_j \rangle_{data}
  -
  \langle v_i h_j \rangle_{recon}
  )
\end{eqnarray}

ここで$\epsilon$は学習率、
$\langle v_i h_j \rangle_{data}$は

データにより特徴検出器が実行されるときに
画素$i$と特徴検出器$j$が同時に起きた回数の極一部で、
そして$\langle v_i h_j \rangle_{recon}$は
'confabulation'のほんの一部について一致している。

その同じ学習則の簡易版は、そのバイアスに使われる。

その学習は良く働くけれども、正確ではなく
訓練データの対数確率の勾配に続いている。

\section{4}

バイナリ特徴の単層は画像のセット内の構築物を
モデル化するためのベストな方法ではない。

特徴検出器のある層を学習後、
私たちはそれらの活動を取り扱うことができる

-データによってそれらを駆動したとき-
特徴の2番目の層の学習についてのデータとして。

そのとき特徴検出器の最初の層は、
次の{\rm RBM}学習について可視素子となる。

この(層による層の,layer-by-layer)学習は
切望に応じて何度でも繰り返すことが出来るだろう。

これは

(余分な層,extra layer)は常にモデルに訓練データが
代入された対数確率の下限で改善されたものが加えられ、

提供される特徴検出器毎層の数は減少し、
そして、それらの重みは正しく初期化される(9)。

モデルに代入される訓練データの対数確率の下限の改善は常に
余分な層に加えられることを示せるだろう、

この境界は高層がより少ない特徴検出器を持つときには適応されないが、

層による層の学習アルゴリズムは、
'deep autoencoder'の重みの事前訓練法にも関わない。

\section{5}

各々の特徴の層は強いものを捉え、
そのモジュールは同様の重みで
初期化されるencoderとdecoderのネットワーク
を生成するために'unfolded(折り畳まれていない)'(図1)。

そのとき、{\rm global fine-tuning stage}
が決定性によって統計的な活動が置換する、

真値確率

そして、最適な復元についての重みを{\rm fine-tune}する
全ての{\rm autoencoder}を通過する誤差逆伝播を使う。



層より下になる素子の活動間の高階の相互関係。

データセットの広範の様々について、
この低次元を前進的に公開(progressive reveal)する方法、非線形構造。



特徴検出器の多層を事前訓練した後、
そのモデルは
同じ重みを使う初期化するためのencoderとdecoderのネットワークを
生成するために'unfolded'(Fig.1)。

{\rm global fine-tuning}ステージは、決定的に統計活動が置き変わり、
真値の確率

そして

最適{\rm reconstruction}についての重みの{\rm fine-tune}するための
全ての'autoencoder'を通る誤差逆伝播を使う。

\section{6}

連続的なデータについて、
(最初の、first)レベルの{\rm RBM}の隱れ素子はバイナリのままであるが、
可視素子はガウシアンノイズをともなう線形素子によって置き換えられる(10)。

もし、このノイズが素子分散を持つならば、
隱れ素子について統計的な更新則は同様のままで、
そして、
素子分散と平均$b_i + \sum_{j} h_j w_{ij}$をともなう
正規分布から抽出するための可視素子$i$についての更新則は
ガウシアンノイズを伴う線形素子によって置換される。

\section{7}

我々の全ての実験において、
全ての{\rm RBM}の可視素子は実数値の活動を持ち、
それはロジスティック素子について$[0,1]$の範囲内である。

高レベルの{\rm RBM}を訓練している間中、
その可視素子は前の{\rm RBM}内で
隱れ素子の活性化確率へ向けられる、

しかし、トップの1つを除いた全ての{\rm RBM}の隱れ素子は
統計的なバイナリ値を持つ。

そのトップの{\rm RBM}の隱れ素子は
{\rm RBM}のもつロジスティック可視素子からの入力により
決定される平均の正規分布の分散素子から描かれる統計的な実数値状態を持つ。

これは連続的な値の使用し、
そして、{\rm PCA}を伴なった比較を促進した。

事前訓練の詳細と{\rm fine-tuning}は(8)で見つけることができる。

\section{8}

事前訓練のアルゴリズムは私たちに{\rm fine-tune}するための
{\rm deep network}を効果的なデモンストレーションし、

私たちは2次元でランダムに選択される3点から生成された
'curves'画像を含んでいる合成データセットで
とても深い{\rm autoencoder}を訓練した(8)。

このデータセットについて、
真の(固有,intrinsic)の次元は既知であり、そして、

ピクセルの明度と6つの数の関係は高階の非線形な
それらを生成するために使われる。

{\rm 0と1}の範囲におさまるピクセルの明度、そして、
それはとても非正規、

だから、{\rm autoencoder}でロジスティック出力素子を使った、

そして、{\rm fine-tuning}する学習のステージを最小化した
クロスエントロピーエラー
\begin{eqnarray}
 \left[
  -\sum_{i} p_i \log \hat p_i 
  -\sum_i (1 - p_i) \log (1 - \hat p_i)
 \right]
\end{eqnarray}

ここで、$p_i$は画素$i$の明度、
そして$\hat p_i$は、それの{\rm reconstruction}の明度である。

\section{9}
その{\rm autoencoder}は
{\rm (28x28)-400-200-100-50-25-6}サイズの層を伴う
{\rm encoder}を構成し、そして、対称なdecoderだ。

そのコード層での6素子は線形で、そして、
その他の素子は全てロジスティックだ。

そのネットワークは$20,000$画像で訓練され、
そして、$10,000$の新しい画像でテストした。

その{\rm autoencoder}は各{\rm 784}ピクセルの画像を
ほとんど完全な復元を許す6つの実数へ変換する方法を発見する(図2A)。

{\rm PCA}は、より悪い復元を与えた。

事前訓練無しで、とても深い{\rm autoencoder}は
常に訓練データの平均を復元し、
なおかつ、曳索された{\rm fine-tuning}(8)。

データとコード間の単層の隱れ層をともなう浅い{\rm autoencoder}は、
事前学習無しで学習することが出来る、
しかし、事前訓練はそれら全ての訓練時間を非常に減少する(8)。

パラメータ数が同じとき、{\rm deep autoencoder}は
テストデータ上の復元エラーは浅いものより低く生成できる、
しかし、この長所はパラメータ数の増加にともない消失する(8)。

\section{10}

次に、{\rm MNIST}訓練セットで全ての手書き数字について抽出したコードへの
{\rm 784-1000-500-250-30 autoencoder}を使う(11)。

事前訓練と{\rm fine-tuning}に使用した{\rm Matlab}コードは(8)に在る。

再び、コード層で30線形素子についてを除けば
全ての素子はロジスティックだ。

全$60,000$訓練画像で{\rm fine-tuning}した後、
その{\rm autoencoder}は新しい画像{\rm 10,000}でテストされ、
そして、{\rm PCA}したものよりずっと良い復元が生成される。(Fig.2B)

2次元{\rm autoencoder}は2つの主要な
構成の最初より良いデータの可視化を生成した。(Fig.3)

\section{11}

私たちは

{\rm Olivetti}顔画像データセットからグレイスケール画像パッチについて
30次元を発見するための線形素子を伴なう
{\rm 625-2000-1000-500-30 autoencoder}使った(12)。

その{\rm autoencoder}は明らかに{\rm PCA}よりパフォーマンスが優れている。
%(outperform,~よりパフォーマンスが優れている)

\section{12}
% whe trained on document
ドキュメントで訓練したとき、
{\rm autoencoder}は速い回復を許すコードを生成する。

我々は{\rm newswire story 804,414}のそれぞれを
2000の{\rm commonest word stem}の確率のドキュメント指定ベクトルとして
提示し(13)、
そして、
{\rm fine-tuning}についてマルチクラスのクロスエントロピー関数
\begin{eqnarray}
 \sum_{i} p_i \log \hat p_i
\end{eqnarray}
の使用を伴なうストーリーの半分で
{\rm 2000-500-250-125-10 autoencoder}
を訓練した。
その10コード素子は線形で、そして隱れ素子はロジスティックのままである。

2つの測度を使ったコード間の角度の余弦が同様の測度として使われたとき、
その{\rm autoencoder}は{\rm latent semantic analysis(LSA)}より
明らかにパフォーマンスが優れてい(14)、
よく知られているドキュメント回復手法は{\rm PCA}をベースにしている。(Fig.4)

{\rm autoencoder}(8)もまた{\rm local linear embedding}より
パフォーマンスが優れており、最近の非線形次元はアルゴリズムで減少する。(15)


\section{13}
層による層の事前訓練もまた分類と回帰に使うことができる。

{\rm MNIST}手書き数字認識処理の幅広く使われている版で、

その最良なエラー率の報告は
ランダムに初期化したバックプロパゲーションで$1.6\%$、
サポートベクターマシンで$1.4\%$だ。

{\rm 784-500-500-2000-10}ネットワークで層による層の事前訓練後、
緩やかな勾配と小さな学習率を用いることは$1.2\%$に至る。

事前訓練は一般化を助ける、なぜならば
モデリングイメージに由来する重みの情報の全体。

そのラベルでとても制限されたの情報は事前訓練によって発見される重みを
ちょうど、わずかにだけ使用される。

\section{14}
これは
深い{\rm autoencoder}を通るバックプロパゲーションは
非線形次元の減少についてとても効果的であることは1980年代から明らかで、

計算機は十分に高速に動作することが提供され、
訓練セットは十分に大きく、そして初期重みは良い解答に十分に近い。
全ての3つの状態は、今、十分である。

ノンパラメトリックな手法とは異なり(15,16)、
{\rm autoencoder}はデータとコード空間の間
も
それらが提供される広範のデータセットもマッピングが与えられ、

そして、{\rm fine-tuning}のスケールは訓練ケースの数を伴なう時間と空間で
線形だ。


\end{document}